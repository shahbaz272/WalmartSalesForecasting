{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_scorer(estimator,X,y):\n",
    "    weight_array = np.where(X.IsHoliday==True,5,1)\n",
    "    preds = estimator.predict(X)\n",
    "    error = np.mean(weight_array*np.abs(y-preds))\n",
    "    \n",
    "    return -(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_features(df):\n",
    "    \n",
    "    df['quarter'] = df['Date'].dt.quarter\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['year'] = df['Date'].dt.year\n",
    "    #df['dayofyear'] = df['Date'].dt.dayofyear\n",
    "    df['weekofyear'] = df['Date'].dt.weekofyear\n",
    "    df['day'] = df['Date'].dt.day\n",
    "    df['days'] = (df.month-1) * 30 + df.day\n",
    "    df['tDays'] = (df.year-2010)*360 + df.days\n",
    "\n",
    "    def week_of_month(dt):\n",
    "        \"\"\" Returns the week of the month for the specified date.\n",
    "        \"\"\"\n",
    "\n",
    "        first_day = dt.replace(day=1)\n",
    "\n",
    "        dom = dt.day\n",
    "        adjusted_dom = dom + first_day.weekday()\n",
    "\n",
    "        return int(ceil(adjusted_dom/7.0))\n",
    "    \n",
    "    df['weekofmonth'] = df['Date'].apply(week_of_month)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(train,test,cat_features):\n",
    "    ohe = OneHotEncoder()\n",
    "    \n",
    "    ohe_train = ohe.fit_transform(train[cat_features]).toarray()\n",
    "    ohe_train = pd.DataFrame(ohe_train,columns=ohe.get_feature_names())\n",
    "    train.drop(cat_features,axis=1,inplace=True)\n",
    "    train.reset_index(drop=True,inplace=True)\n",
    "    train = pd.concat([train,ohe_train],axis=1)\n",
    "    \n",
    "    ohe_test = ohe.transform(test[cat_features]).toarray()\n",
    "    ohe_test = pd.DataFrame(ohe_test,columns=ohe.get_feature_names())\n",
    "    test.drop(cat_features,axis=1,inplace=True)\n",
    "    test.reset_index(drop=True,inplace=True)\n",
    "    test = pd.concat([test,ohe_test],axis=1)\n",
    "    \n",
    "    return (train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(train,test,lags):\n",
    "    \n",
    "    train['set']='Train'\n",
    "    test['set']='Test'\n",
    "    test['Weekly_Sales']=0\n",
    "    \n",
    "    one_df=pd.concat([train,test],ignore_index=True)\n",
    "    \n",
    "    lags = range(39, lags+1)\n",
    "\n",
    "    df = one_df.assign(**{'Weekly_sales(t-{})'.format(t): one_df.groupby(['Store','Dept']).Weekly_Sales.shift(t) for t in lags})\n",
    "    \n",
    "    train_set = df[df.set=='Train']\n",
    "    test_set = df[df.set=='Test']\n",
    "\n",
    "    \n",
    "    return train_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_other_features(df):\n",
    "    df['IsHoliday_bin']=np.where(df.IsHoliday,1,0)\n",
    "    \n",
    "    #type_dict = {'A':1,'B':2,'C':3}\n",
    "    #df['Type_num']=df.Type.map(type_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputing(train,test,num_features):\n",
    "    imputer = SimpleImputer(strategy='constant',fill_value=0)\n",
    "    \n",
    "\n",
    "    imp_train = pd.DataFrame(imputer.fit_transform(train[num_features]),columns=num_features)\n",
    "    train[num_features] = imp_train\n",
    "    \n",
    "    test.reset_index(drop=True,inplace=True)\n",
    "    imp_test = pd.DataFrame(imputer.transform(test[num_features]),columns=num_features)\n",
    "    test[num_features] = imp_test\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(preds,org,weights):\n",
    "    org_preds = np.exp(preds)-4990\n",
    "    error = np.mean(weights*np.abs(org-org_preds))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_log_loss_scorer(estimator,X,y):\n",
    "    weight_array = np.where(X.IsHoliday_num==1,5,1)\n",
    "    log_preds = estimator.predict(X)\n",
    "    org_preds = np.exp(log_preds)-4990\n",
    "    org_y = np.exp(y)-4990\n",
    "    error = np.mean(weight_array*np.abs(org_y-org_preds))\n",
    "    \n",
    "    return -(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(preds,org,weights):\n",
    "    error = np.mean(weights*np.abs(preds-org))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imports, Cleaning and Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('walmart-recruiting-store-sales-forecasting/features.csv',parse_dates=[1])\n",
    "stores = pd.read_csv('walmart-recruiting-store-sales-forecasting/stores.csv')\n",
    "test = pd.read_csv('walmart-recruiting-store-sales-forecasting/test.csv',parse_dates=[2])\n",
    "train = pd.read_csv('walmart-recruiting-store-sales-forecasting/train.csv',parse_dates=[2])\n",
    "submission = pd.read_csv('walmart-recruiting-store-sales-forecasting/sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(left=pd.merge(left=train,right=stores,how='left'),right=features,how='left',on=['Store','Date','IsHoliday'])\n",
    "test_data = pd.merge(left = pd.merge(left=test,right=stores,how='left'),right=features,how='left',on = ['Store','Date','IsHoliday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_date_features(train_data)\n",
    "test_data = create_date_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = one_hot_encoding(train_data,test_data,['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = create_lag_features(train_data,test_data,52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_other_features(train_data)\n",
    "test_data = create_other_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = list(train_data.columns[np.isin(train_data.dtypes,['int32','int64','float64'])])\n",
    "train_data,test_data = imputing(train_data,test_data,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['log_sales'] = np.log(4990+train_data.Weekly_Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Dept'] = train_data['Dept'].map(int)\n",
    "train_data['Store'] = train_data['Store'].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Store:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \n",
      "Fold: 1\n",
      "Store:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \n",
      "Fold: 2\n",
      "Store:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 \n"
     ]
    }
   ],
   "source": [
    "train_rf_model = True\n",
    "tscv = TimeSeriesSplit()\n",
    "cv_data = dict()\n",
    "fold=0\n",
    "for train_index, test_index in tscv.split(train_data.Date.unique()):\n",
    "    train_dates = train_data.Date.unique()[train_index]\n",
    "    test_dates = train_data.Date.unique()[test_index]\n",
    "    train_data_cv = train_data.loc[train_data.Date.isin(train_dates)]\n",
    "    test_data_cv = train_data.loc[train_data.Date.isin(test_dates)]\n",
    "    \n",
    "    print('Fold:',fold)\n",
    "\n",
    "    features_list = ['Size','x0_A','x0_B','x0_C','year','month','day','days','IsHoliday_bin','tDays',\n",
    "                     'Weekly_sales(t-47)',\n",
    "                     'Weekly_sales(t-48)',\n",
    "                     'Weekly_sales(t-49)',\n",
    "                    'Weekly_sales(t-50)', \n",
    "                     'Weekly_sales(t-51)', \n",
    "                     'Weekly_sales(t-52)']\n",
    "\n",
    "    train_stores = train_data_cv.Store.unique()\n",
    "    test_stores = test_data_cv.Store.unique()\n",
    "\n",
    "    detailed_data = dict()\n",
    "    print(\"Store:\")\n",
    "    for test_store in test_stores:\n",
    "\n",
    "        \n",
    "        train_depts = train_data_cv.loc[train_data_cv.Store==test_store].Dept.unique()\n",
    "        test_depts = test_data_cv.loc[test_data_cv.Store==test_store].Dept.unique()\n",
    "\n",
    "        print(str(test_store),end=' ')\n",
    "\n",
    "        for test_dept in test_depts:\n",
    "\n",
    "            #print(str(test_dept),end = \" \")        \n",
    "            less_than_10 = False\n",
    "\n",
    "            train_store_dept_data = train_data_cv.loc[(train_data_cv.Store==test_store) & (train_data_cv.Dept == test_dept)]\n",
    "            test_store_dept_data = test_data_cv[(test_data_cv.Store==test_store) & (test_data_cv.Dept==test_dept)]\n",
    "\n",
    "            if (len(train_store_dept_data)<10):\n",
    "\n",
    "                train_store_dept_data = train_data_cv.loc[train_data_cv.Dept == test_dept]\n",
    "                test_store_dept_data = test_data_cv.loc[test_data_cv.Dept == test_dept]        \n",
    "                less_than_10 = True\n",
    "\n",
    "            X = train_store_dept_data[features_list]\n",
    "            y = train_store_dept_data.log_sales\n",
    "            weights = np.where(X.IsHoliday_bin==1,5,1)\n",
    "\n",
    "            #XGB Model\n",
    "            model_xgb = XGBRegressor(verbosity=0,njobs=-1, random_state=42)\n",
    "            model_xgb.fit(X,y,sample_weight=weights)\n",
    "\n",
    "            train_preds_xgb = model_xgb.predict(X)\n",
    "            test_preds_xgb = model_xgb.predict(test_store_dept_data[features_list])\n",
    "\n",
    "            if(train_rf_model):\n",
    "            ##RF Model\n",
    "                model_rf = RandomForestRegressor(n_estimators=500,n_jobs=-1,verbose=0,random_state=42)\n",
    "                model_rf.fit(X,y)\n",
    "\n",
    "                train_preds_rf = model_rf.predict(X)\n",
    "                test_preds_rf = model_rf.predict(test_store_dept_data[features_list])\n",
    "\n",
    "            if (less_than_10):\n",
    "\n",
    "                if(len(train_data_cv.loc[(train_data_cv.Store==test_store) & (train_data_cv.Dept == test_dept)][features_list])>0):\n",
    "                    train_preds_xgb = model_xgb.predict(train_data_cv.loc[(train_data_cv.Store==test_store) & (train_data_cv.Dept == test_dept)][features_list])\n",
    "                    if(train_rf_model):\n",
    "                        train_preds_rf = model_rf.predict(train_data_cv.loc[(train_data_cv.Store==test_store) & (train_data_cv.Dept == test_dept)][features_list])\n",
    "                else:\n",
    "                    train_preds_xgb = None\n",
    "                    if(train_rf_model):\n",
    "                        train_preds_rf = None\n",
    "\n",
    "                if(len(test_data_cv.loc[(test_data_cv.Store==test_store) & (test_data_cv.Dept==test_dept)][features_list])>0):\n",
    "                    test_preds_xgb = model_xgb.predict(test_data_cv[(test_data_cv.Store==test_store) & (test_data_cv.Dept==test_dept)][features_list])\n",
    "                    if(train_rf_model):\n",
    "                        test_preds_rf = model_rf.predict(test_data_cv[(test_data_cv.Store==test_store) & (test_data_cv.Dept==test_dept)][features_list])\n",
    "\n",
    "                else:\n",
    "                    test_preds_xgb = None\n",
    "                    if(train_rf_model):\n",
    "                        test_preds_rf = None\n",
    "\n",
    "            current_data = dict()\n",
    "            #current_data['train_X'] = X\n",
    "            #current_data['train_y'] = y\n",
    "            #current_data['test_X'] = test_store_dept_data[features_list]\n",
    "            current_data['model_xgb'] = None\n",
    "            current_data['model_rf'] = None  ##RF models take up a lot of memory. My system crashes if I store them.\n",
    "            #current_data['less_data'] = less_than_10\n",
    "            current_data['train_preds_xgb'] = train_preds_xgb\n",
    "            current_data['test_preds_xgb'] = test_preds_xgb\n",
    "            if(train_rf_model):\n",
    "                current_data['train_preds_rf'] = train_preds_rf\n",
    "                current_data['test_preds_rf'] = test_preds_rf\n",
    "\n",
    "            detailed_data[str(test_store) + '_' + str(test_dept)] = current_data\n",
    "    cv_data['Fold_' + str(fold)] = detailed_data\n",
    "    fold+=1\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_preds = dict()\n",
    "\n",
    "for i in range(fold):\n",
    "    \n",
    "    cv_preds['train_'+str(i)+'_xgb'] = [preds for dept_data in cv_data['Fold_'+str(i)].values() if dept_data['train_preds_xgb'] is not None for preds in dept_data['train_preds_xgb']]\n",
    "    cv_preds['test_'+str(i)+'_xgb'] = [preds for dept_data in cv_data['Fold_'+str(i)].values() if dept_data['test_preds_xgb'] is not None for preds in dept_data['test_preds_xgb']]\n",
    "    \n",
    "    cv_preds['train_'+str(i)+'_rf'] = [preds for dept_data in cv_data['Fold_'+str(i)].values() if dept_data['train_preds_rf'] is not None for preds in dept_data['train_preds_rf']]\n",
    "    cv_preds['test_'+str(i)+'_rf'] = [preds for dept_data in cv_data['Fold_'+str(i)].values() if dept_data['test_preds_rf'] is not None for preds in dept_data['test_preds_rf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "xgb_train_errors = list()\n",
    "xgb_test_errors = list()\n",
    "\n",
    "rf_train_errors = list()\n",
    "rf_test_errors = list()\n",
    "\n",
    "\n",
    "for train_index, test_index in tscv.split(train_data.Date.unique()):\n",
    "    train_dates = train_data.Date.unique()[train_index]\n",
    "    test_dates = train_data.Date.unique()[test_index]\n",
    "    train_data_cv = train_data.loc[train_data.Date.isin(train_dates)]\n",
    "    test_data_cv = train_data.loc[train_data.Date.isin(test_dates)]\n",
    "        \n",
    "    redundant_pairs_mask = (~(train_data_cv.Store.map(str) +'-'+ train_data_cv.Dept.map(str)).isin(test_data_cv.Store.map(str) +'-'+ test_data_cv.Dept.map(str)))\n",
    "    train_data_red = train_data_cv[~redundant_pairs_mask]\n",
    "    \n",
    "    xgb_train_error = log_error(cv_preds['train_'+str(fold)+'_xgb'],train_data_red.Weekly_Sales,weights=train_data_red.IsHoliday_bin)\n",
    "    xgb_test_error = log_error(cv_preds['test_'+str(fold)+'_xgb'],test_data_cv.Weekly_Sales,weights=test_data_cv.IsHoliday_bin)\n",
    "    \n",
    "    rf_train_error = log_error(cv_preds['train_'+str(fold)+'_rf'],train_data_red.Weekly_Sales,weights=train_data_red.IsHoliday_bin)\n",
    "    rf_test_error = log_error(cv_preds['test_'+str(fold)+'_rf'],test_data_cv.Weekly_Sales,weights=test_data_cv.IsHoliday_bin)\n",
    "    \n",
    "    xgb_train_errors.append(xgb_train_error)\n",
    "    xgb_test_errors.append(xgb_test_error)\n",
    "    \n",
    "    rf_train_errors.append(rf_train_error)\n",
    "    rf_test_errors.append(rf_test_error)\n",
    "    \n",
    "    fold+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB_Train_errors:                                          Mean_error:       Std_error\n",
      "[3.290811150682096, 11.272404003876083, 15.85434554242053] 10.139186898992904 5.191257706195322\n",
      "XGB_Test_errors:                                          Mean_error:       Std_error\n",
      "[414.2752093873054, 267.9907874002783, 46.12734707612813] 242.79778128790392 151.34778447625408\n",
      "RF_Train_errors:                                            Mean_error:       Std_error\n",
      "[30.166075184351413, 118.95751625358109, 107.07433252181693] 85.39930798658315 39.355939985723985\n",
      "RF_Test_errors:                                             Mean_error:       Std_error\n",
      "[412.40456154267395, 341.9688249653949, 46.50078555532682] 266.9580573544652 158.51678094896948\n"
     ]
    }
   ],
   "source": [
    "print('XGB_Train_errors:                                          Mean_error:       Std_error')\n",
    "print(xgb_train_errors,np.mean(xgb_train_errors),np.std(xgb_train_errors))\n",
    "print('XGB_Test_errors:                                          Mean_error:       Std_error')\n",
    "print(xgb_test_errors,np.mean(xgb_test_errors),np.std(xgb_test_errors))\n",
    "\n",
    "print('RF_Train_errors:                                            Mean_error:       Std_error')\n",
    "print(rf_train_errors,np.mean(rf_train_errors),np.std(rf_train_errors))\n",
    "print('RF_Test_errors:                                             Mean_error:       Std_error')\n",
    "print(rf_test_errors,np.mean(rf_test_errors),np.std(rf_test_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Store: 1.0\n",
      "\n",
      "Store: 2.0\n",
      "\n",
      "Store: 3.0\n",
      "\n",
      "Store: 4.0\n",
      "\n",
      "Store: 5.0\n",
      "\n",
      "Store: 6.0\n",
      "\n",
      "Store: 7.0\n",
      "\n",
      "Store: 8.0\n",
      "\n",
      "Store: 9.0\n",
      "\n",
      "Store: 10.0\n",
      "\n",
      "Store: 11.0\n",
      "\n",
      "Store: 12.0\n",
      "\n",
      "Store: 13.0\n",
      "\n",
      "Store: 14.0\n",
      "\n",
      "Store: 15.0\n",
      "\n",
      "Store: 16.0\n",
      "\n",
      "Store: 17.0\n",
      "\n",
      "Store: 18.0\n",
      "\n",
      "Store: 19.0\n",
      "\n",
      "Store: 20.0\n",
      "\n",
      "Store: 21.0\n",
      "\n",
      "Store: 22.0\n",
      "\n",
      "Store: 23.0\n",
      "\n",
      "Store: 24.0\n",
      "\n",
      "Store: 25.0\n",
      "\n",
      "Store: 26.0\n",
      "\n",
      "Store: 27.0\n",
      "\n",
      "Store: 28.0\n",
      "\n",
      "Store: 29.0\n",
      "\n",
      "Store: 30.0\n",
      "\n",
      "Store: 31.0\n",
      "\n",
      "Store: 32.0\n",
      "\n",
      "Store: 33.0\n",
      "\n",
      "Store: 34.0\n",
      "\n",
      "Store: 35.0\n",
      "\n",
      "Store: 36.0\n",
      "\n",
      "Store: 37.0\n",
      "\n",
      "Store: 38.0\n",
      "\n",
      "Store: 39.0\n",
      "\n",
      "Store: 40.0\n",
      "\n",
      "Store: 41.0\n",
      "\n",
      "Store: 42.0\n",
      "\n",
      "Store: 43.0\n",
      "\n",
      "Store: 44.0\n",
      "\n",
      "Store: 45.0\n"
     ]
    }
   ],
   "source": [
    "validation=False\n",
    "\n",
    "features_list = ['Size','x0_A','x0_B','x0_C','year','month','day','days','IsHoliday_bin','tDays',\n",
    "                 'Weekly_sales(t-47)',\n",
    "                 'Weekly_sales(t-48)',\n",
    "                 'Weekly_sales(t-49)',\n",
    "                'Weekly_sales(t-50)', \n",
    "                 'Weekly_sales(t-51)', \n",
    "                 'Weekly_sales(t-52)']\n",
    "\n",
    "train_stores = train_data.Store.unique()\n",
    "test_stores = test_data.Store.unique()\n",
    "\n",
    "detailed_data = dict()\n",
    "\n",
    "for test_store in test_stores:\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    train_depts = train_data.loc[train_data.Store==test_store].Dept.unique()\n",
    "    test_depts = test_data.loc[test_data.Store==test_store].Dept.unique()\n",
    "    \n",
    "    print('Store: ' + str(test_store))\n",
    "    \n",
    "    for test_dept in test_depts:\n",
    "        \n",
    "        # print(str(test_dept),end = \" \")        \n",
    "        less_than_10 = False\n",
    "\n",
    "        train_store_dept_data = train_data.loc[(train_data.Store==test_store) & (train_data.Dept == test_dept)]\n",
    "        test_store_dept_data = test_data[(test_data.Store==test_store) & (test_data.Dept==test_dept)]\n",
    "        \n",
    "        if (len(train_store_dept_data)<10):\n",
    "            \n",
    "            train_store_dept_data = train_data.loc[train_data.Dept == test_dept]\n",
    "            test_store_dept_data = test_data.loc[test_data.Dept == test_dept]        \n",
    "            less_than_10 = True\n",
    "        \n",
    "        X = train_store_dept_data[features_list]\n",
    "        y = train_store_dept_data.log_sales\n",
    "        weights = np.where(X.IsHoliday_bin==1,5,1)\n",
    "        \n",
    "        #XGB Model\n",
    "        model_xgb = XGBRegressor(verbosity=0,njobs=-1, random_state=42)\n",
    "        model_xgb.fit(X,y,sample_weight=weights)\n",
    "        \n",
    "        train_preds_xgb = model_xgb.predict(X)\n",
    "        test_preds_xgb = model_xgb.predict(test_store_dept_data[features_list])\n",
    "        \n",
    "        ##RF Model\n",
    "        model_rf = RandomForestRegressor(n_estimators=500,n_jobs=-1,verbose=0,random_state=42)\n",
    "        model_rf.fit(X,y)\n",
    "        \n",
    "        train_preds_rf = model_rf.predict(X)\n",
    "        test_preds_rf = model_rf.predict(test_store_dept_data[features_list])\n",
    "        \n",
    "        if (less_than_10):\n",
    "            \n",
    "            if(len(train_data.loc[(train_data.Store==test_store) & (train_data.Dept == test_dept)][features_list])>0):\n",
    "                train_preds_xgb = model_xgb.predict(train_data.loc[(train_data.Store==test_store) & (train_data.Dept == test_dept)][features_list])\n",
    "                train_preds_rf = model_rf.predict(train_data.loc[(train_data.Store==test_store) & (train_data.Dept == test_dept)][features_list])\n",
    "            else:\n",
    "                train_preds_xgb = None\n",
    "                train_preds_rf = None\n",
    "            \n",
    "            if(len(test_data.loc[(test_data.Store==test_store) & (test_data.Dept==test_dept)][features_list])>0):\n",
    "                test_preds_xgb = model_xgb.predict(test_data[(test_data.Store==test_store) & (test_data.Dept==test_dept)][features_list])\n",
    "                test_preds_rf = model_rf.predict(test_data[(test_data.Store==test_store) & (test_data.Dept==test_dept)][features_list])\n",
    "            \n",
    "            else:\n",
    "                test_preds_xgb = None\n",
    "                test_preds_rf = None\n",
    "                \n",
    "        current_data = dict()\n",
    "        current_data['train_X'] = X\n",
    "        current_data['train_y'] = y\n",
    "        current_data['test_X'] = test_store_dept_data[features_list]\n",
    "        #current_data['test_y'] = test_y\n",
    "        current_data['model_xgb'] = model_xgb\n",
    "        current_data['model_rf'] = None  ##RF models take up a lot of memory. My system crashes if I store them.\n",
    "        current_data['less_data'] = less_than_10\n",
    "        current_data['train_preds_xgb'] = train_preds_xgb\n",
    "        current_data['test_preds_xgb'] = test_preds_xgb\n",
    "        current_data['train_preds_rf'] = train_preds_rf\n",
    "        current_data['test_preds_rf'] = test_preds_rf\n",
    "        \n",
    "        detailed_data[str(test_store) + '_' + str(test_dept)] = current_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_preds_xgb = [preds for dept_data in detailed_data.values() if dept_data['train_preds_xgb'] is not None for preds in dept_data['train_preds_xgb']]\n",
    "all_train_preds_rf = [preds for dept_data in detailed_data.values() if dept_data['train_preds_rf'] is not None for preds in dept_data['train_preds_rf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_preds_xgb = [preds for dept_data in detailed_data.values() for preds in dept_data['test_preds_xgb']]\n",
    "all_test_preds_rf = [preds for dept_data in detailed_data.values() for preds in dept_data['test_preds_rf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing redundant pairs in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_pairs_mask = (~(train_data.Store.map(int).map(str) +'-'+ train_data.Dept.map(int).map(str)).isin(test_data.Store.map(int).map(str) +'-'+ test_data.Dept.map(int).map(str)))\n",
    "train_data_red = train_data[~redundant_pairs_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_error : 78.1344048443614\n",
      "XGB_error : 15.70580154454777\n"
     ]
    }
   ],
   "source": [
    "rf_error = log_error(all_train_preds_rf,train_data_red.Weekly_Sales,weights=train_data_red.IsHoliday_bin)\n",
    "xgb_error = log_error(all_train_preds_xgb,train_data_red.Weekly_Sales,weights=train_data_red.IsHoliday_bin)\n",
    "print('RF_error :',rf_error)\n",
    "print('XGB_error :',xgb_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of predictions from all models to the test/train tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_xgb_test_preds = pd.read_csv('big_xgb_test_preds.csv').Weekly_Sales\n",
    "test_data['preds_xgb']=np.exp(all_test_preds_xgb)-4990\n",
    "test_data['preds_rf']=np.exp(all_test_preds_rf)-4990\n",
    "test_data['preds_big_xgb'] = big_xgb_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['preds'] = np.mean(test_data[['preds_xgb','preds_rf','preds_big_xgb']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahb\\AnacondaPython\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\shahb\\AnacondaPython\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\shahb\\AnacondaPython\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\shahb\\AnacondaPython\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "big_xgb_train_preds = pd.read_csv('big_xgb_train_preds.csv').preds\n",
    "train_data_red['preds_xgb']=np.exp(all_train_preds_xgb)-4990\n",
    "train_data_red['preds_rf']=np.exp(all_train_preds_rf)-4990\n",
    "train_data_red['preds_big_xgb'] = big_xgb_train_preds[~redundant_pairs_mask]\n",
    "train_data_red['preds'] = np.mean(train_data_red[['preds_xgb','preds_rf','preds_big_xgb']],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustment of Weekly Sales during the christmas holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Store:')\n",
    "for store in t.Store.unique():\n",
    "\n",
    "    print('store',end=\" \")\n",
    "    for dept in t.Dept.unique():\n",
    "        subset = t.loc[(t.Store==store)&(t.Dept==dept)&(t.weekofyear.isin([48,49,50,51,52]))]\n",
    "        if (len(subset)<5):\n",
    "            continue\n",
    "\n",
    "        old_preds = np.array(subset.preds)\n",
    "        pre_mean = old_preds[1:4].mean()\n",
    "        post_mean = (old_preds[0]+old_preds[-1])/2\n",
    "        \n",
    "        if ((pre_mean/post_mean)>1.1):\n",
    "        \n",
    "            shifted = old_preds * (7-2.5)/7\n",
    "            shifted[1:] = np.array(shifted[1:5]) + np.array(old_preds[0:4]) * (2.5/7)\n",
    "            shifted[0] = old_preds[0]\n",
    "            t.loc[(t.Store==store)&(t.Dept==dept)&(t.weekofyear.isin([48,49,50,51,52])),'preds']=shifted\n",
    "            #print(dept,end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Weekly_Sales'] = t.reset_index().preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('final_preds.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_red['abs_diff'] = np.abs(train_data_red.Weekly_Sales - train_data_red.preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error(train_data_red.preds,train_data_red.Weekly_Sales,train_data_red.IsHoliday_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,9))\n",
    "train_data_red.groupby(['Dept']).apply(lambda x: error(x['preds'],x['Weekly_Sales'],x['IsHoliday_bin'])).plot(kind='bar');\n",
    "plt.title('Department Wise MSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,9))\n",
    "train_data_red.groupby(['Store']).apply(lambda x: error(x['preds'],x['Weekly_Sales'],x['IsHoliday_bin'])).plot(kind='bar');\n",
    "plt.title('Store wise MSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = pd.DataFrame(train_data_red.groupby(['Dept','Store']).apply(lambda x: error(x['preds'],x['Weekly_Sales'],x['IsHoliday_bin'])),columns=['WMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mat = train_errors.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mat.columns = range(1,46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,18))\n",
    "hm = sns.heatmap(error_mat)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
